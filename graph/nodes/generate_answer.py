# nodes/generate_answer.py
import json
from openai import OpenAI
from graph.state import SelfRAGState

client = OpenAI()


def calculate_llm_score(answer: str, context: str, relevance_score: float) -> float:
    """
    LLM ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
    - ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜
    - ë‹µë³€ ê¸¸ì´ í‰ê°€ (ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì )
    """
    # ê¸°ë³¸ ì ìˆ˜ëŠ” ê´€ë ¨ì„± ì ìˆ˜ì—ì„œ ì‹œì‘
    base_score = relevance_score if relevance_score > 0 else 0.70

    # ë‹µë³€ ê¸¸ì´ í‰ê°€ (ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì )
    answer_length = len(answer)
    if answer_length < 50:
        length_penalty = 0.20
    elif answer_length < 100:
        length_penalty = 0.10
    else:
        length_penalty = 0.0

    # ìµœì¢… ì ìˆ˜ ê³„ì‚°
    final_score = base_score - length_penalty

    # 0.0 ~ 1.0 ë²”ìœ„ë¡œ ì œí•œí•˜ê³  ì†Œìˆ˜ì  2ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
    return round(max(0.0, min(1.0, final_score)), 2)


def generate_answer(state: SelfRAGState) -> SelfRAGState:
    """
    í†µí•© ë‹µë³€ ìƒì„± ë…¸ë“œ
    - ë¹„ì˜í•™ ì§ˆë¬¸: ì•ˆë‚´ ë©”ì‹œì§€
    - ì˜í•™ ìš©ì–´ ì§ˆë¬¸: WebSearch ê²°ê³¼ ê¸°ë°˜ ë‹µë³€
    - ì¼ë°˜ ì˜í•™ ì§ˆë¬¸: RAG ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
    """

    # ì‹œì‘ ë¡œê·¸
    query = state.get("question", "")
    context_len = len(state.get("context", ""))
    is_terminology = state.get("is_terminology", False)
    print(f"â€¢ [Generate] start (context_chars={context_len}, is_terminology={is_terminology})")

    # 1. ë¹„ì˜í•™ ì§ˆë¬¸ ì²˜ë¦¬ (guidance ë¡œì§)
    if state.get("need_quit", False):
        state["final_answer"] = """
ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì€ ì˜í•™ ê´€ë ¨ ì§ˆë¬¸ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì˜í•™, ê±´ê°•, ì§ˆë³‘, ì¦ìƒ, ì¹˜ë£Œ ë“±ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ì˜ˆì‹œ:
- "ë‹¹ë‡¨ë³‘ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
- "ê³ í˜ˆì••ì˜ ì¦ìƒì€ ë¬´ì—‡ì¸ê°€ìš”?"
- "ë…ê° ì˜ˆë°©ì ‘ì¢…ì€ ì–¸ì œ ë°›ëŠ” ê²ƒì´ ì¢‹ë‚˜ìš”?"
        """.strip()
        return state

    # 2. ì˜í•™ ì§ˆë¬¸ ì²˜ë¦¬
    query = state.get("question", "")
    context = state.get("context", "")
    sources = state.get("sources", [])
    is_terminology = state.get("is_terminology", False)

    # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
    if not context:
        if is_terminology:
            state["final_answer"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            state["final_answer"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return state

    # 3. WebSearch ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ (answer_websearch ë¡œì§)
    if is_terminology:
        prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì„¤ëª…í•˜ê³ , í•„ìš”í•œ ê²½ìš° ìƒì„¸ ì„¤ëª…ì„ ì´ì–´ì„œ ì‘ì„±í•˜ì„¸ìš”.

ì¤‘ìš” ì‘ì„± ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ì˜í•™ ì •ë³´ëŠ” ì‹ ì¤‘í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”
- ê¸´ ë¬¸ì„œë“¤ì€ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•˜ì—¬ ì¤‘ìš” ì •ë³´ë“¤ë§Œ ì „ë‹¬í•´ì£¼ì„¸ìš”
- ë²ˆí˜¸ë‚˜ êµ¬ì¡°í™”ëœ í˜•ì‹ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
- í•µì‹¬ ë‹¨ì–´ì— ** markdown ê°•ì¡° í‘œí˜„ì„ ì ìš©í•˜ì„¸ìš”.
        """

        res = client.chat.completions.create(
            model="gpt-5-nano",
            messages=[{"role": "user", "content": prompt}]
        )

        answer = res.choices[0].message.content.strip()

        # LLM ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        llm_score = calculate_llm_score(answer, context, state.get("relevance_score", 0.0))

        # JSON êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
        state["structured_answer"] = {
            "answer": answer,
            "references": sources,  # sources ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            "llm_score": llm_score,
            "relevance_score": round(state.get("relevance_score", 0.0), 2)
        }
        state["llm_score"] = llm_score

        # ë‹µë³€ ëì— ì°¸ê³ ë¬¸ì„œ ëª©ë¡ ì¶”ê°€ (í‰ë¬¸ìš©)
        if sources:
            sources_text = "\n\nğŸ“š ì°¸ê³ ë¬¸ì„œ:\n" + "\n".join(f"- {src}" for src in sources)
            state["final_answer"] = answer + sources_text
        else:
            state["final_answer"] = answer

    # 4. RAG ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ (answer_rag ë¡œì§)
    else:
        prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ ë¬¸ì„œ:
{context}

ìœ„ ë¬¸ì„œë¥¼ ê·¼ê±°ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì„¤ëª…í•˜ê³ , í•„ìš”í•œ ê²½ìš° ìƒì„¸ ì„¤ëª…ê³¼ ì£¼ì˜ì‚¬í•­ì„ ì´ì–´ì„œ ì‘ì„±í•˜ì„¸ìš”.

ì¤‘ìš” ì‘ì„± ê·œì¹™:
- ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ë‹µë³€ ë³¸ë¬¸ì— ë¬¸ì„œ ë²ˆí˜¸([1], [2] ë“±)ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì˜í•™ ì •ë³´ëŠ” ì‹ ì¤‘í•˜ê³  ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”
- ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¬¸ì„œ ë‚´ìš©ì— ì¶©ì‹¤í•˜ì„¸ìš”
- ë²ˆí˜¸ë‚˜ êµ¬ì¡°í™”ëœ í˜•ì‹ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
- í•µì‹¬ ë‹¨ì–´ì— ** markdown ê°•ì¡° í‘œí˜„ì„ ì ìš©í•˜ì„¸ìš”.
        """

        res = client.chat.completions.create(
            model="gpt-5-nano",
            messages=[{"role": "user", "content": prompt}]
        )

        answer = res.choices[0].message.content.strip()

        # LLM ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        llm_score = calculate_llm_score(answer, context, state.get("relevance_score", 0.0))

        # JSON êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
        state["structured_answer"] = {
            "answer": answer,
            "references": sources,  # sources ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            "llm_score": llm_score,
            "relevance_score": round(state.get("relevance_score", 0.0), 2)
        }
        state["llm_score"] = llm_score

    # ì™„ë£Œ ë¡œê·¸
    answer_len = len(state.get("final_answer", ""))
    print(f"â€¢ [Generate] complete (answer_chars={answer_len})")

    return state
