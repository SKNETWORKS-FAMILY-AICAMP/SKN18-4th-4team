# nodes/generate_answer.py
import json
from openai import OpenAI
from graph.state import SelfRAGState

client = OpenAI()


def calculate_llm_score(answer: str, context: str, relevance_score: float) -> float:
    """
    LLM ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
    - ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜
    - ë‹µë³€ ê¸¸ì´ í‰ê°€ (ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì )
    """
    # ê¸°ë³¸ ì ìˆ˜ëŠ” ê´€ë ¨ì„± ì ìˆ˜ì—ì„œ ì‹œì‘
    base_score = relevance_score if relevance_score > 0 else 0.70

    # ë‹µë³€ ê¸¸ì´ í‰ê°€ (ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì )
    answer_length = len(answer)
    if answer_length < 50:
        length_penalty = 0.20
    elif answer_length < 100:
        length_penalty = 0.10
    else:
        length_penalty = 0.0

    # ìµœì¢… ì ìˆ˜ ê³„ì‚°
    final_score = base_score - length_penalty

    # 0.0 ~ 1.0 ë²”ìœ„ë¡œ ì œí•œí•˜ê³  ì†Œìˆ˜ì  2ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
    return round(max(0.0, min(1.0, final_score)), 2)


def generate_answer_i(state: SelfRAGState) -> SelfRAGState:
    """
    í†µí•© ë‹µë³€ ìƒì„± ë…¸ë“œ (ê°œì„  ë²„ì „)
    - ë¹„ì˜í•™ ì§ˆë¬¸: LLMì´ ëŒ€í™” ì´ë ¥ì„ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ë‹µë³€ ìƒì„±
    - ì˜í•™ ìš©ì–´ ì§ˆë¬¸: WebSearch ê²°ê³¼ ê¸°ë°˜ ë‹µë³€
    - ì¼ë°˜ ì˜í•™ ì§ˆë¬¸: RAG ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
    - conversation_historyë¥¼ í™œìš©í•˜ì—¬ ë§¥ë½ ì¸ì‹ ë‹µë³€ ìƒì„±
    """

    # ì‹œì‘ ë¡œê·¸
    query = state.get("question", "")
    context_len = len(state.get("context", ""))
    is_terminology = state.get("is_terminology", False)
    need_quit = state.get("need_quit", False)
    print(f"â€¢ [Generate] start (context_chars={context_len}, is_terminology={is_terminology}, need_quit={need_quit})")

    # ëŒ€í™” ì´ë ¥ ê°€ì ¸ì˜¤ê¸°
    conversation_history = state.get("conversation_history", {})
    history_summary = conversation_history.get("summary", "")
    last_conversation = conversation_history.get("last_conversation", "")
    facts = conversation_history.get("facts", [])

    # 1. ë¹„ì˜í•™ ì§ˆë¬¸ ì²˜ë¦¬ - LLMì´ ì ì ˆí•œ ë‹µë³€ ìƒì„±
    if state.get("need_quit", False):
        print("â€¢ [Generate] Processing non-medical question with LLM")

        # ëŒ€í™” ì´ë ¥ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì§ì „ ëŒ€í™” ìš°ì„ )
        history_context = ""
        if last_conversation:
            history_context = f"""
ì§ì „ ëŒ€í™”:
{last_conversation}

"""

        # ì „ì²´ ëŒ€í™” ì´ë ¥ ì¶”ê°€ (ì°¸ê³ ìš©)
        if history_summary and len(history_summary) > len(last_conversation or ""):
            history_context += f"""
ì´ì „ ëŒ€í™” ì´ë ¥ (ì°¸ê³ ):
{history_summary}

"""

        # ì£¼ìš” ì‚¬ì‹¤ ì¶”ê°€
        if facts:
            history_context += f"""
ì‚¬ìš©ì ì •ë³´:
{", ".join(facts)}

"""

        prompt = f"""{history_context}ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì¤‘ìš” ì‘ì„± ê·œì¹™:
- ì§ˆë¬¸ì— ëŒ€ëª…ì‚¬("ì´ëŸ¬í•œ", "ê·¸ê²ƒ", "ì €ê²ƒ", "ì´", "ê·¸", "ì €" ë“±)ê°€ ìˆìœ¼ë©´ **ì§ì „ ëŒ€í™”**ë¥¼ ìš°ì„  ì°¸ê³ í•˜ì„¸ìš”
- ì§ì „ ëŒ€í™”ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì´ì „ ëŒ€í™” ì´ë ¥ì„ ì°¸ê³ í•˜ì„¸ìš”
- ì‚¬ìš©ì ì •ë³´(ì´ë¦„, ì·¨ë¯¸ ë“±)ê°€ ìˆìœ¼ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜í•˜ì„¸ìš”
- ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ëŒ€í™” í†¤ì„ ìœ ì§€í•˜ì„¸ìš”
- ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
        """

        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )

        answer = res.choices[0].message.content.strip()
        state["final_answer"] = answer

        # ë¹„ì˜í•™ ì§ˆë¬¸ì€ structured_answerë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŒ
        # ì™„ë£Œ ë¡œê·¸
        answer_len = len(answer)
        print(f"â€¢ [Generate] complete (answer_chars={answer_len}, non-medical)")

        return state

    # 2. ì˜í•™ ì§ˆë¬¸ ì²˜ë¦¬
    query = state.get("question", "")
    context = state.get("context", "")
    sources = state.get("sources", [])
    is_terminology = state.get("is_terminology", False)

    # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
    if not context:
        if is_terminology:
            state["final_answer"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            state["final_answer"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return state

    # 3. WebSearch ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ (answer_websearch ë¡œì§)
    if is_terminology:
        # ëŒ€í™” ì´ë ¥ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì§ì „ ëŒ€í™” ìš°ì„ )
        history_context = ""
        if last_conversation:
            history_context = f"""
ì§ì „ ëŒ€í™”:
{last_conversation}

"""

        # ì „ì²´ ëŒ€í™” ì´ë ¥ ì¶”ê°€ (ì°¸ê³ ìš©)
        if history_summary and len(history_summary) > len(last_conversation or ""):
            history_context += f"""
ì´ì „ ëŒ€í™” ì´ë ¥ (ì°¸ê³ ):
{history_summary}

"""

        # ì£¼ìš” ì‚¬ì‹¤ ì¶”ê°€
        if facts:
            history_context += f"""
ì‚¬ìš©ì ì •ë³´:
{", ".join(facts)}

"""

        prompt = f"""{history_context}ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê²€ìƒ‰ëœ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì„¤ëª…í•˜ê³ , í•„ìš”í•œ ê²½ìš° ìƒì„¸ ì„¤ëª…ì„ ì´ì–´ì„œ ì‘ì„±í•˜ì„¸ìš”.

ì¤‘ìš” ì‘ì„± ê·œì¹™:
- ê²€ìƒ‰ ê²°ê³¼ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ì§ˆë¬¸ì— ëŒ€ëª…ì‚¬("ì´ëŸ¬í•œ", "ê·¸ê²ƒ", "ì €ê²ƒ" ë“±)ê°€ ìˆìœ¼ë©´ **ì§ì „ ëŒ€í™”**ë¥¼ ìš°ì„  ì°¸ê³ í•˜ì„¸ìš”
- ì§ì „ ëŒ€í™”ì—ì„œ ë§¥ë½ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì´ì „ ëŒ€í™” ì´ë ¥ì„ ì°¸ê³ í•˜ì„¸ìš”
- ì‚¬ìš©ì ì •ë³´(ì´ë¦„ ë“±)ê°€ ìˆìœ¼ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜í•˜ì„¸ìš”
- ë‹µë³€ ë³¸ë¬¸ì— ì¶œì²˜ ë²ˆí˜¸([1], [2] ë“±)ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì˜í•™ ì •ë³´ëŠ” ì‹ ì¤‘í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”
- ê¸´ ë¬¸ì„œë“¤ì€ ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•˜ì—¬ ì¤‘ìš” ì •ë³´ë“¤ë§Œ ì „ë‹¬í•´ì£¼ì„¸ìš”
- ë²ˆí˜¸ë‚˜ êµ¬ì¡°í™”ëœ í˜•ì‹ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
        """

        res = client.chat.completions.create(
            model="gpt-5-nano",
            messages=[{"role": "user", "content": prompt}]
        )

        answer = res.choices[0].message.content.strip()

        # LLM ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        llm_score = calculate_llm_score(answer, context, state.get("relevance_score", 0.0))

        # JSON êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
        state["structured_answer"] = {
            "answer": answer,
            "references": sources,  # sources ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            "llm_score": llm_score,
            "relevance_score": round(state.get("relevance_score", 0.0), 2)
        }
        state["llm_score"] = llm_score

        # ë‹µë³€ ëì— ì°¸ê³ ë¬¸ì„œ ëª©ë¡ ì¶”ê°€ (í‰ë¬¸ìš©)
        if sources:
            sources_text = "\n\nğŸ“š ì°¸ê³ ë¬¸ì„œ:\n" + "\n".join(f"- {src}" for src in sources)
            state["final_answer"] = answer + sources_text
        else:
            state["final_answer"] = answer

    # 4. RAG ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ (answer_rag ë¡œì§)
    else:
        # ëŒ€í™” ì´ë ¥ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì§ì „ ëŒ€í™” ìš°ì„ )
        history_context = ""
        if last_conversation:
            history_context = f"""
ì§ì „ ëŒ€í™”:
{last_conversation}

"""

        # ì „ì²´ ëŒ€í™” ì´ë ¥ ì¶”ê°€ (ì°¸ê³ ìš©)
        if history_summary and len(history_summary) > len(last_conversation or ""):
            history_context += f"""
ì´ì „ ëŒ€í™” ì´ë ¥ (ì°¸ê³ ):
{history_summary}

"""

        # ì£¼ìš” ì‚¬ì‹¤ ì¶”ê°€
        if facts:
            history_context += f"""
ì‚¬ìš©ì ì •ë³´:
{", ".join(facts)}

"""

        prompt = f"""{history_context}ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ ë¬¸ì„œ:
{context}

ìœ„ ë¬¸ì„œë¥¼ ê·¼ê±°ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì„¤ëª…í•˜ê³ , í•„ìš”í•œ ê²½ìš° ìƒì„¸ ì„¤ëª…ê³¼ ì£¼ì˜ì‚¬í•­ì„ ì´ì–´ì„œ ì‘ì„±í•˜ì„¸ìš”.

ì¤‘ìš” ì‘ì„± ê·œì¹™:
- ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ì§ˆë¬¸ì— ëŒ€ëª…ì‚¬("ì´ëŸ¬í•œ", "ê·¸ê²ƒ", "ì €ê²ƒ" ë“±)ê°€ ìˆìœ¼ë©´ **ì§ì „ ëŒ€í™”**ë¥¼ ìš°ì„  ì°¸ê³ í•˜ì„¸ìš”
- ì§ì „ ëŒ€í™”ì—ì„œ ë§¥ë½ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì´ì „ ëŒ€í™” ì´ë ¥ì„ ì°¸ê³ í•˜ì„¸ìš”
- ì‚¬ìš©ì ì •ë³´(ì´ë¦„ ë“±)ê°€ ìˆìœ¼ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜í•˜ì„¸ìš”
- ë‹µë³€ ë³¸ë¬¸ì— ë¬¸ì„œ ë²ˆí˜¸([1], [2] ë“±)ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì˜í•™ ì •ë³´ëŠ” ì‹ ì¤‘í•˜ê³  ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”
- ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¬¸ì„œ ë‚´ìš©ì— ì¶©ì‹¤í•˜ì„¸ìš”
- ë²ˆí˜¸ë‚˜ êµ¬ì¡°í™”ëœ í˜•ì‹ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
        """

        res = client.chat.completions.create(
            model="gpt-5-nano",
            messages=[{"role": "user", "content": prompt}]
        )

        answer = res.choices[0].message.content.strip()

        # LLM ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        llm_score = calculate_llm_score(answer, context, state.get("relevance_score", 0.0))

        # JSON êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
        state["structured_answer"] = {
            "answer": answer,
            "references": sources,  # sources ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            "llm_score": llm_score,
            "relevance_score": round(state.get("relevance_score", 0.0), 2)
        }
        state["llm_score"] = llm_score

    # ì™„ë£Œ ë¡œê·¸
    answer_len = len(state.get("final_answer", ""))
    print(f"â€¢ [Generate] complete (answer_chars={answer_len})")

    return state
