import os
import sys
from dotenv import load_dotenv
from pathlib import Path

# =========================
# 1. í™˜ê²½ ì„¤ì •
# =========================
load_dotenv()

BASE_DIR = Path(__file__).resolve().parent.parent.parent
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

# =========================
# 2. ì „ì²˜ë¦¬ ë° ì²­í‚¹ ë‹¨ê³„
# =========================
from rag.etl.transform.cleaner import main as clean_main
from rag.etl.transform.chunker import main as chunk_main

print("ğŸš€ [1ë‹¨ê³„] ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
clean_main()
print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!\n")

print("ğŸš€ [2ë‹¨ê³„] ì²­í‚¹(chunking) ì‹œì‘...")
chunk_main()
print("âœ… ì²­í‚¹ ì™„ë£Œ!\n")

# =========================
# 3. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ ë‹¨ê³„
# =========================
from rag.etl.load.csvloader import CustomCSVLoader
from rag.services.db_pool import DatabasePool
from rag.services.vectorstore_pg import CustomPGVector
from langchain_openai import OpenAIEmbeddings


def add_documents_in_batches(store, docs, batch_size=100):
    total = len(docs)
    for i in range(0, total, batch_size):
        batch = docs[i:i + batch_size]
        print(f"ğŸ”¹ ì„ë² ë”© ì¤‘... {i + len(batch)}/{total}")
        store.add_documents(batch)
    print("âœ… ì „ì²´ ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ.")


def main():
    print("ğŸš€ [3ë‹¨ê³„] CSV ë¡œë“œ ë° ì„ë² ë”© ì‹œì‘...")

    loader = CustomCSVLoader(
        file_path="rag/data/T3_chunked2.csv",
        content_columns=["chunk_text"],
        metadata_columns=["c_id"]
    )
    docs = loader.load()
    if not docs:
        print("âš ï¸ CSVì—ì„œ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    db = DatabasePool()
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    store = CustomPGVector(db=db, embedding_fn=embeddings, table="medical")

    add_documents_in_batches(store, docs, batch_size=100)

    print("ğŸ¯ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
